
# Solver/optimizer configuration

# Optimization
optimizer: adamw
lr: 1.0e-4
weight_decay: 0.01
betas: [0.9, 0.999]
eps: 1.0e-8

# Learning rate schedule
lr_scheduler: cosine
warmup_steps: 1000
min_lr: 1.0e-6

# Training
epochs: 10
gradient_clip: 1.0
accumulation_steps: 1

# Validation
validate_every: 1
save_every: 1

# Mixed precision
use_amp: false
